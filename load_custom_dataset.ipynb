{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "from datasets import concatenate_datasets, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Generator, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"llava-hf/LLaVa-NeXT-Video-7b-hf\"\n",
    "NUM_FRAMES = 8\n",
    "MAX_LENGTH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decord import VideoReader, gpu, cpu\n",
    "\n",
    "def read_video_decord(video_path, num_frames=NUM_FRAMES):\n",
    "    '''\n",
    "    Decode the video with Decord decoder.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        num_frames (int): Number of frames to sample uniformly. Defaults to NUM_FRAMES\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    vr = VideoReader(uri=video_path, ctx=cpu(0)) # you need to install from source to use gpu ctx\n",
    "    indices = np.arange(0, len(vr), len(vr) / num_frames).astype(int)\n",
    "    frames = vr.get_batch(indices).asnumpy()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qa(qa_path: Path) -> pd.DataFrame:\n",
    "    # read annotations from disk\n",
    "\n",
    "    with qa_path.open(\"r\") as f:\n",
    "        qa = json.load(f)\n",
    "\n",
    "    samples = []\n",
    "    for entry in qa:\n",
    "        for i, conversation in enumerate(entry[\"conversations\"]):\n",
    "            assert conversation[0][\"from\"] == \"human\"\n",
    "            assert conversation[1][\"from\"] == \"gpt\"\n",
    "\n",
    "            samples.append(\n",
    "                {\n",
    "                    \"video\": entry[\"video\"],\n",
    "                    \"messages\": conversation,\n",
    "                    \"question_id\": f\"{entry['video'][:-4]}_{i}\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    df = pd.DataFrame(samples)\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_batches(video_path: Path, qa_path: Path) -> Generator[Dict, None, None]:\n",
    "    df = load_qa(qa_path)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        sample = {\n",
    "            \"video_path\": Path(video_path).joinpath(row.video).resolve().as_posix(),\n",
    "            \"text_prompt\": row.messages[0][\"value\"],\n",
    "            \"target_answer\": row.messages[1][\"value\"],\n",
    "            \"question_id\": row.question_id,\n",
    "        }\n",
    "        \n",
    "        yield sample\n",
    "        if idx >= 1000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We collate to save everything in tensor format to speed-up dataloading process\n",
    "# Saving the whole video clip (array) along with caption (string) will slow down iteration\n",
    "# because unprocessed video clip will take up more memory due to higher resolution\n",
    "# The processed video on the other hand is always 336x336 in size and fixed frame count per clip\n",
    "# see: https://discuss.huggingface.co/t/slow-iteration-speed-with-and-without-keep-in-memory-true/33587\n",
    "\n",
    "\n",
    "def collate_fn(sample, processor):\n",
    "    video_clip = read_video_decord(\n",
    "        sample[\"video_path\"]\n",
    "    )  # change to the video decoder you want\n",
    "\n",
    "    # Let's use chat template to format the prompt correctly\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": sample[\"text_prompt\"]},\n",
    "                {\"type\": \"video\"},\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": sample[\"target_answer\"]},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=False)\n",
    "\n",
    "    batch = processor(\n",
    "        text=prompt,\n",
    "        videos=video_clip,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = Path(\"/data/msrvtt-qa/videos\")\n",
    "qa_path = Path(\"/data/msrvtt-qa/qa.json\")\n",
    "\n",
    "ds = Dataset.from_generator(\n",
    "    prepare_batches, gen_kwargs={\"video_path\": video_path, \"qa_path\": qa_path}\n",
    ")\n",
    "\n",
    "# ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "# And we also need to load the processor for collate_fn\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, use_fast=False)\n",
    "processor.tokenizer.padding_side = \"right\" # during training, one always uses padding on the right\n",
    "\n",
    "dataset = ds.map(collate_fn, batched=False, fn_kwargs={\"processor\": processor}, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the datasets we have and load a tokenizer\n",
    "# dataset_processed = concatenate_datasets(datasets_combined)\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = dataset['train'].with_format(\"torch\"), dataset['test'].with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlavaNextVideoDataCollatorWithPadding:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, features):\n",
    "        padded_inputs = self.processor.tokenizer.pad(\n",
    "            {\n",
    "                \"input_ids\": [feat['input_ids'][0] for feat in features], # each element is one batch only so we slice [0]\n",
    "                \"attention_mask\": [feat['attention_mask'][0] for feat in features],\n",
    "            },\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels = padded_inputs[\"input_ids\"].clone()\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "        padded_inputs[\"labels\"] = labels\n",
    "        padded_inputs[\"pixel_values_videos\"] = torch.cat([feat['pixel_values_videos'] for feat in features], dim=0)\n",
    "\n",
    "        return padded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# convert to image from proceessed tensors\n",
    "clip = example[\"pixel_values_videos\"][0] * 255\n",
    "clip = clip.permute(0, 2, 3, 1).clamp(0, 255)\n",
    "\n",
    "# np array with shape (frames, height, width, channels)\n",
    "video = np.array(clip).astype(np.uint8)\n",
    "\n",
    "fig = plt.figure()\n",
    "im = plt.imshow(video[0,:,:,:])\n",
    "\n",
    "plt.close() # this is required to not display the generated image\n",
    "\n",
    "def init():\n",
    "    im.set_data(video[0,:,:,:])\n",
    "\n",
    "def animate(i):\n",
    "    im.set_data(video[i,:,:,:])\n",
    "    return im\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=video.shape[0],\n",
    "                               interval=100)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the caption associated with the video clip\n",
    "processor.batch_decode(example[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training_toolkit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
