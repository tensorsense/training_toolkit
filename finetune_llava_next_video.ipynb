{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import av\n",
    "import fsspec\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, Seq2SeqTrainingArguments, DataCollatorForLanguageModeling\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig, LlavaNextVideoForConditionalGeneration\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from huggingface_hub import snapshot_download, hf_hub_download, HfFileSystem\n",
    "from datasets import load_dataset, concatenate_datasets, load_from_disk\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 4\n",
    "NUM_FRAMES = 8 # more frames -> more VRAM needed\n",
    "DATASET_PATH = \"/data/sharegpt4video\" # path where to save the dataset\n",
    "OUTPUT_DIR = \"/data/training_toolkit/outputs\" # path where to save the checkpoints\n",
    "MODEL_ID = \"llava-hf/LLaVa-NeXT-Video-7b-hf\"\n",
    "# REPO_ID = \"RaushanTurganbay/LLaVa-NeXT-Video-demo\" # Change to your hf-hub repo\n",
    "\n",
    "USE_LORA = False\n",
    "USE_QLORA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fetch_dataset import fetch_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we also need to load the processor for collate_fn\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, use_fast=False)\n",
    "processor.tokenizer.padding_side = \"right\" # during training, one always uses padding on the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the datasets we have and load a tokenizer\n",
    "dataset_processed = fetch_dataset(\n",
    "    video_path=Path(\"/data/msrvtt-qa/videos\"),\n",
    "    qa_path=Path(\"/data/msrvtt-qa/qa.json\"),\n",
    "    processor=processor,\n",
    ")\n",
    "dataset_processed = dataset_processed.shuffle(seed=42)\n",
    "dataset = dataset_processed.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk(\"msrvtt_1000.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = dataset['train'].with_format(\"torch\"), dataset['test'].with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demo purposes only a small portion of the dataset was downloaded\n",
    "# The whole dataset has 40k videos\n",
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlavaNextVideoDataCollatorWithPadding:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, features):\n",
    "        padded_inputs = self.processor.tokenizer.pad(\n",
    "            {\n",
    "                \"input_ids\": [feat['input_ids'][0] for feat in features], # each element is one batch only so we slice [0]\n",
    "                \"attention_mask\": [feat['attention_mask'][0] for feat in features],\n",
    "            },\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels = padded_inputs[\"input_ids\"].clone()\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "        padded_inputs[\"labels\"] = labels\n",
    "        padded_inputs[\"pixel_values_videos\"] = torch.cat([feat['pixel_values_videos'] for feat in features], dim=0)\n",
    "\n",
    "        return padded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# convert to image from proceessed tensors\n",
    "clip = example[\"pixel_values_videos\"][0] * 255\n",
    "clip = clip.permute(0, 2, 3, 1).clamp(0, 255)\n",
    "\n",
    "# np array with shape (frames, height, width, channels)\n",
    "video = np.array(clip).astype(np.uint8)\n",
    "\n",
    "fig = plt.figure()\n",
    "im = plt.imshow(video[0,:,:,:])\n",
    "\n",
    "plt.close() # this is required to not display the generated image\n",
    "\n",
    "def init():\n",
    "    im.set_data(video[0,:,:,:])\n",
    "\n",
    "def animate(i):\n",
    "    im.set_data(video[i,:,:,:])\n",
    "    return im\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=video.shape[0],\n",
    "                               interval=100)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the caption associated with the video clip\n",
    "processor.batch_decode(example[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load model\n",
    "# Three options for training, from the lowest precision training to the highest precision training:\n",
    "# QLoRA: model uses 4-bit quantization, which helps in reducing memory usage while maintaining performance.\n",
    "# Standard LoRA:  model is loaded with standard LoRA adaptations.\n",
    "# Full Fine-Tuning: no memory optimization are done. In that case Flash Attention is used to speed up training, if hardware supports it.\n",
    "\n",
    "if USE_QLORA or USE_LORA:\n",
    "    if USE_QLORA:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "        )\n",
    "    model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "else:\n",
    "    # for full fine-tuning, we can speed up the model using Flash Attention\n",
    "    # only available on certain devices, see https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features\n",
    "    model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        _attn_implementation=\"flash_attention_2\",\n",
    "        device_map=\"auto\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    multimodal_keywords = ['multi_modal_projector', 'vision_model']\n",
    "    for name, module in model.named_modules():\n",
    "        if any(mm_keyword in name for mm_keyword in multimodal_keywords):\n",
    "            continue\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=find_all_linear_names(model),\n",
    "    init_lora_weights=\"gaussian\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "\n",
    "    # args related to training\n",
    "    output_dir = OUTPUT_DIR,\n",
    "    eval_strategy = 'steps',\n",
    "    eval_steps=20,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size = BATCH_SIZE,\n",
    "    gradient_accumulation_steps = 8,\n",
    "    learning_rate = 2e-05,\n",
    "    max_steps = 100, # adjust this depending on your dataset size\n",
    "    lr_scheduler_type = 'cosine',\n",
    "    warmup_ratio = 0.1,\n",
    "\n",
    "    # args related to eval/save\n",
    "    logging_steps = 20,\n",
    "    save_strategy = 'steps',\n",
    "    save_steps=20,\n",
    "    save_total_limit = 1,\n",
    "    fp16 = True, # we have the model train and eval with fp16 precision\n",
    "    fp16_full_eval = True,\n",
    "    optim = 'adamw_bnb_8bit', # adam in lower-bits to save memory, consider changing to 'adamw_torch' if model is not converging\n",
    "    # report_to = \"wandb\", # install wand to use this\n",
    "    # hub_model_id = REPO_ID,\n",
    "    # push_to_hub = True, # wel'll push the model to hub after each epoch\n",
    "\n",
    "    # model that was wrapped for QLORA training with peft will not have arguments listed in its signature\n",
    "    # so we need to pass lable names explicitly to calculate val loss\n",
    "    label_names=[\"labels\"],\n",
    "    dataloader_num_workers=4, # let's get more workers since iterating on video datasets might be slower in general\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    tokenizer = processor,\n",
    "    data_collator = LlavaNextVideoDataCollatorWithPadding(processor=processor),\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset,\n",
    "    args=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
    "    f\"{OUTPUT_DIR}/checkpoint-40\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"msrvtt_1000.hf\")\n",
    "train_dataset, test_dataset = dataset['train'].with_format(\"torch\"), dataset['test'].with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we also need to load the processor for collate_fn\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, use_fast=False)\n",
    "processor.tokenizer.padding_side = \"right\" # during training, one always uses padding on the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = test_dataset[0]\n",
    "\n",
    "# convert to image from proceessed tensors\n",
    "clip = example[\"pixel_values_videos\"][0] * 255\n",
    "clip = clip.permute(0, 2, 3, 1).clamp(0, 255)\n",
    "\n",
    "# np array with shape (frames, height, width, channels)\n",
    "video = np.array(clip).astype(np.uint8)\n",
    "\n",
    "fig = plt.figure()\n",
    "im = plt.imshow(video[0,:,:,:])\n",
    "\n",
    "plt.close() # this is required to not display the generated image\n",
    "\n",
    "def init():\n",
    "    im.set_data(video[0,:,:,:])\n",
    "\n",
    "def animate(i):\n",
    "    im.set_data(video[i,:,:,:])\n",
    "    return im\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=video.shape[0],\n",
    "                               interval=100)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.batch_decode(example[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(video_clip, model):\n",
    "    # Let's use chat template to format the prompt correctly, this time without the caption\n",
    "    conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Provide a detailed caption for this video.\"},\n",
    "                    {\"type\": \"video\"},\n",
    "                    ],\n",
    "            },\n",
    "        ]\n",
    "\n",
    "    # Set add_generation_prompt to add the \"ASSISTANT: \" at the end\n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "    batch = processor(\n",
    "        text=prompt,\n",
    "        videos=None, # we have a processed video, passing it again to processor causes errors\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    video_clip = video_clip.to(model.device)\n",
    "\n",
    "    out = model.generate(**batch, pixel_values_videos=video_clip, max_length=MAX_LENGTH, do_sample=True)\n",
    "    generated_text = processor.batch_decode(out, skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(example[\"pixel_values_videos\"], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(example[\"pixel_values_videos\"], old_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training_toolkit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
