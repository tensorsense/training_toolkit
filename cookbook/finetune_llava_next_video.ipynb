{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append(Path(\"..\").resolve().as_posix())\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_toolkit import build_trainer, llava_next_video_preset, local_video_preset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = build_trainer(\n",
    "    **local_video_preset.as_kwargs(),\n",
    "    **llava_next_video_preset.as_kwargs(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
    "    f\"{OUTPUT_DIR}/checkpoint-40\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"msrvtt_1000.hf\")\n",
    "train_dataset, test_dataset = dataset['train'].with_format(\"torch\"), dataset['test'].with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# convert to image from proceessed tensors\n",
    "clip = example[\"pixel_values_videos\"][0] * 255\n",
    "clip = clip.permute(0, 2, 3, 1).clamp(0, 255)\n",
    "\n",
    "# np array with shape (frames, height, width, channels)\n",
    "video = np.array(clip).astype(np.uint8)\n",
    "\n",
    "fig = plt.figure()\n",
    "im = plt.imshow(video[0,:,:,:])\n",
    "\n",
    "plt.close() # this is required to not display the generated image\n",
    "\n",
    "def init():\n",
    "    im.set_data(video[0,:,:,:])\n",
    "\n",
    "def animate(i):\n",
    "    im.set_data(video[i,:,:,:])\n",
    "    return im\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=video.shape[0],\n",
    "                               interval=100)\n",
    "HTML(anim.to_html5_video())\n",
    "\n",
    "# and the caption associated with the video clip\n",
    "processor.batch_decode(example[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we also need to load the processor for collate_fn\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, use_fast=False)\n",
    "processor.tokenizer.padding_side = \"right\" # during training, one always uses padding on the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = test_dataset[0]\n",
    "\n",
    "# convert to image from proceessed tensors\n",
    "clip = example[\"pixel_values_videos\"][0] * 255\n",
    "clip = clip.permute(0, 2, 3, 1).clamp(0, 255)\n",
    "\n",
    "# np array with shape (frames, height, width, channels)\n",
    "video = np.array(clip).astype(np.uint8)\n",
    "\n",
    "fig = plt.figure()\n",
    "im = plt.imshow(video[0,:,:,:])\n",
    "\n",
    "plt.close() # this is required to not display the generated image\n",
    "\n",
    "def init():\n",
    "    im.set_data(video[0,:,:,:])\n",
    "\n",
    "def animate(i):\n",
    "    im.set_data(video[i,:,:,:])\n",
    "    return im\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=video.shape[0],\n",
    "                               interval=100)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.batch_decode(example[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(video_clip, model):\n",
    "    # Let's use chat template to format the prompt correctly, this time without the caption\n",
    "    conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Provide a detailed caption for this video.\"},\n",
    "                    {\"type\": \"video\"},\n",
    "                    ],\n",
    "            },\n",
    "        ]\n",
    "\n",
    "    # Set add_generation_prompt to add the \"ASSISTANT: \" at the end\n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "    batch = processor(\n",
    "        text=prompt,\n",
    "        videos=None, # we have a processed video, passing it again to processor causes errors\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    video_clip = video_clip.to(model.device)\n",
    "\n",
    "    out = model.generate(**batch, pixel_values_videos=video_clip, max_length=MAX_LENGTH, do_sample=True)\n",
    "    generated_text = processor.batch_decode(out, skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(example[\"pixel_values_videos\"], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(example[\"pixel_values_videos\"], old_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training_toolkit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
