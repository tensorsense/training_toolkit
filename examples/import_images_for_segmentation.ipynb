{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append(Path(\"..\").resolve().as_posix())\n",
    "_ = load_dotenv()\n",
    "\n",
    "# import supervision as sv\n",
    "from datasets import Dataset, Image\n",
    "from collections import defaultdict\n",
    "import PIL\n",
    "\n",
    "# from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import cv2\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose(\n",
    "    [\n",
    "        A.SmallestMaxSize(max_size=386, always_apply=True),\n",
    "        A.CenterCrop(height=386, width=386, always_apply=True),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(\n",
    "        format=\"pascal_voc\", label_fields=[\"class_labels\"], clip=True, min_area=1\n",
    "    ),\n",
    ")\n",
    "\n",
    "dataset_path = Path(\"/data/trash_demo/TACO/data/\")\n",
    "coco = COCO(dataset_path.joinpath(\"annotations.json\").as_posix())\n",
    "\n",
    "\n",
    "image_ids = coco.getImgIds()\n",
    "categories = [coco.cats[cat_id][\"name\"] for cat_id in coco.getCatIds()]\n",
    "\n",
    "dataset_dict = defaultdict(list)\n",
    "prefix = \"segment \" + \" ; \".join(categories)\n",
    "\n",
    "\n",
    "for image_id in tqdm(image_ids):\n",
    "    image_path = dataset_path.joinpath(coco.loadImgs(image_id)[0][\"file_name\"])\n",
    "    annotations = coco.loadAnns(coco.getAnnIds(image_id))\n",
    "    xywh_bboxes = [ann[\"bbox\"] for ann in annotations]\n",
    "    xyxy_bboxes = [[x, y, x + w, y + h] for x, y, w, h in xywh_bboxes]\n",
    "    classes = [categories[ann[\"category_id\"]] for ann in annotations]\n",
    "\n",
    "    image = cv2.imread(image_path.as_posix())\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    masks = [coco.annToMask(ann) for ann in annotations]\n",
    "\n",
    "    transformed = transform(\n",
    "        image=image, masks=masks, bboxes=xyxy_bboxes, class_labels=classes\n",
    "    )\n",
    "\n",
    "    image = PIL.Image.fromarray(transformed[\"image\"])\n",
    "    masks = np.array(transformed[\"masks\"], dtype=bool)\n",
    "    xyxy_bboxes = np.array(transformed[\"bboxes\"], dtype=int)\n",
    "    classes = transformed[\"class_labels\"]\n",
    "\n",
    "    xyxy_bboxes = np.array([[x1, y1, x2, y2] for x1, y1, x2, y2 in xyxy_bboxes if x2 - x1 > 0 and y2 - y1 > 0])\n",
    "\n",
    "    if len(masks) == 0 or len(xyxy_bboxes) == 0 or len(classes) == 0:\n",
    "        continue\n",
    "\n",
    "    assert len(masks.shape) == 3\n",
    "    assert (\n",
    "        len(xyxy_bboxes.shape) == 2 and xyxy_bboxes.shape[1] == 4\n",
    "    ), f\"{xyxy_bboxes.shape}, {len(masks)}, {len(xyxy_bboxes)}\"\n",
    "    \n",
    "\n",
    "    dataset_dict[\"image\"].append(image)\n",
    "    dataset_dict[\"prompt\"].append(prefix)\n",
    "    dataset_dict[\"xyxy_bboxes\"].append(xyxy_bboxes)\n",
    "    dataset_dict[\"masks\"].append(masks)\n",
    "    dataset_dict[\"classes\"].append(classes)\n",
    "\n",
    "\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "dataset = dataset.cast_column(\"image\", Image())\n",
    "\n",
    "dataset.info.dataset_name = \"taco_trash\"\n",
    "dataset.info.description = f\"class_names: {' ; '.join(categories)}\"\n",
    "\n",
    "dataset.save_to_disk(\"taco_trash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(Path(\"..\").resolve().as_posix())\n",
    "_ = load_dotenv()\n",
    "\n",
    "from datasets import Dataset\n",
    "import PIL\n",
    "import numpy as np\n",
    "from training_toolkit.src.common.tokenization_utils.segmentation import (\n",
    "    SegmentationTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.load_from_disk(\"taco_trash\")\n",
    "dataset = dataset.with_format(\"torch\")\n",
    "\n",
    "segmentation_tokenizer = SegmentationTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[0]\n",
    "PIL.Image.fromarray(example[\"masks\"][0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[0]\n",
    "suffix = segmentation_tokenizer.encode(\n",
    "    example[\"image\"], example[\"xyxy_bboxes\"], example[\"masks\"], example[\"classes\"]\n",
    ")\n",
    "\n",
    "suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = segmentation_tokenizer.decode(suffix, 386, 386)\n",
    "\n",
    "PIL.Image.fromarray((decoded[0][\"mask\"] > 0.5).astype(np.uint8) * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(Path(\"..\").resolve().as_posix())\n",
    "_ = load_dotenv()\n",
    "\n",
    "from training_toolkit import paligemma_image_preset, image_segmentation_preset, build_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paligemma_image_preset.training_args[\"per_device_train_batch_size\"] = 12\n",
    "paligemma_image_preset.training_args[\"per_device_eval_batch_size\"] = 12\n",
    "paligemma_image_preset.training_args[\"num_train_epochs\"] = 8\n",
    "\n",
    "trainer = build_trainer(\n",
    "    **paligemma_image_preset.as_kwargs(),\n",
    "    **image_segmentation_preset.with_path(\"taco_trash\").as_kwargs()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append(Path(\"..\").resolve().as_posix())\n",
    "_ = load_dotenv()\n",
    "\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoProcessor\n",
    "import PIL\n",
    "import numpy as np\n",
    "import cv2\n",
    "import supervision as sv\n",
    "\n",
    "from training_toolkit.src.common.tokenization_utils.segmentation import (\n",
    "    SegmentationTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"paligemma_2024-08-01_16-13-08/checkpoint-200\"\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(CHECKPOINT_PATH)\n",
    "processor = AutoProcessor.from_pretrained(CHECKPOINT_PATH)\n",
    "segmentation_tokenizer = SegmentationTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"assets/trash1.jpg\")\n",
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"segment trash\"\n",
    "\n",
    "PROMPT = prefix\n",
    "\n",
    "inputs = processor(images=image, text=PROMPT)\n",
    "\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=256, do_sample=True)\n",
    "\n",
    "# Next we turn each predicted token ID back into a string using the decode method\n",
    "# We chop of the prompt, which consists of image tokens and our text prompt\n",
    "image_token_index = model.config.image_token_index\n",
    "num_image_tokens = len(generated_ids[generated_ids == image_token_index])\n",
    "num_text_tokens = len(processor.tokenizer.encode(PROMPT))\n",
    "num_prompt_tokens = num_image_tokens + num_text_tokens + 2\n",
    "generated_text = processor.batch_decode(\n",
    "    generated_ids[:, num_prompt_tokens:],\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=False,\n",
    ")[0]\n",
    "\n",
    "w, h = image.size\n",
    "\n",
    "generated_segmentation = segmentation_tokenizer.decode(generated_text, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL.Image.fromarray((generated_segmentation[0][\"mask\"] > 0.5).astype(np.uint8) * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyxy = []\n",
    "mask = []\n",
    "class_id = []\n",
    "class_name = []\n",
    "\n",
    "for r in generated_segmentation:\n",
    "    xyxy.append(r[\"xyxy\"])\n",
    "    _, m = cv2.threshold(r[\"mask\"], 0.5, 1.0, cv2.THRESH_BINARY)\n",
    "    mask.append(m)\n",
    "    # class_id.append(ds.classes.index(r[\"name\"].strip()))\n",
    "    # class_id.append(classes.index(r['name'].strip()))\n",
    "    class_id.append(0)\n",
    "    class_name.append(r[\"name\"].strip())\n",
    "\n",
    "detections = sv.Detections(\n",
    "    xyxy=np.array(xyxy).astype(int),\n",
    "    mask=np.array(mask).astype(bool),\n",
    "    class_id=np.array(class_id).astype(int),\n",
    ")\n",
    "\n",
    "detections[\"class_name\"] = class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = sv.BoxAnnotator().annotate(image, detections)\n",
    "\n",
    "image = sv.MaskAnnotator().annotate(image, detections)\n",
    "image = sv.LabelAnnotator(text_scale=2, text_thickness=4, text_position=sv.Position.CENTER_OF_MASS, text_color=sv.Color.BLACK).annotate(image, detections)\n",
    "\n",
    "# sv.plot_images_grid([image], (2, 2))\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training_toolkit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "nbformat": 4,
  "nbformat_minor": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
